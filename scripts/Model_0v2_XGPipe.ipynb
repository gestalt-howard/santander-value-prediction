{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# XGBoost with Preprocessing Pipeline\n",
    "Inspired from: https://www.kaggle.com/sggpls/pipeline-kernel-xgb-fe-lb1-39"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xgboost as xgb\n",
    "\n",
    "import dill\n",
    "import pdb\n",
    "\n",
    "from scipy.stats import skew, kurtosis\n",
    "\n",
    "from sklearn.base import BaseEstimator, TransformerMixin, RegressorMixin  # For making custom classes\n",
    "from sklearn.externals.joblib import Parallel, delayed\n",
    "from sklearn.base import clone\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.pipeline import Pipeline, FeatureUnion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "debug=True\n",
    "random_state=0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper Functions and Classes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UniqueTransformer(BaseEstimator, TransformerMixin):\n",
    "    '''\n",
    "    Class with fit and transform methods for removing duplicate columns from a dataset\n",
    "    **fit** finds the indexes of unique columns using numpy unique\n",
    "    **transform** returns the dataset with the indexes of unique columns\n",
    "    '''\n",
    "    def __init__(self, axis=1):\n",
    "        if axis==0:\n",
    "            raise NotImplementedError('Axis is 0! Not implemented!')\n",
    "        self.axis=axis\n",
    "    \n",
    "    def fit(self, X, y=None):\n",
    "        print 'Finding unique indexes...'\n",
    "        _, self.unique_indexes_ = np.unique(X, axis=self.axis, return_index=True)\n",
    "        return self \n",
    "    \n",
    "    def transform(self, X, y=None):\n",
    "        print 'Filtering for only unique columns...'\n",
    "        return X[:, self.unique_indexes_]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ClassifierTransformer(BaseEstimator, TransformerMixin):\n",
    "    '''\n",
    "    Class describing an object that transforms datasets via estimator results\n",
    "    **_get_labels** specifies target value bins and transforms target vector into bin values \n",
    "    '''\n",
    "    def __init__(self, estimator=None, n_classes=2, cv=3):\n",
    "        self.estimator=estimator\n",
    "        self.n_classes=n_classes\n",
    "        self.cv=cv\n",
    "        \n",
    "    def _get_labels(self, y):\n",
    "        y_labels = np.zeros(len(y))\n",
    "        y_us = np.sort(np.unique(y))\n",
    "        step = int(len(y_us)/self.n_classes)\n",
    "        \n",
    "        for i_class in range(self.n_classes):\n",
    "            if i_class+1 == self.n_classes:  # Edge case where i_class is initialized at 1\n",
    "                y_labels[y >= y_us[i_class*step]] = i_class\n",
    "            else:\n",
    "                y_labels[np.logical_and(y>=y_us[i_class*step], y<y_us[(i_class+1)*step])] = i_class\n",
    "        return y_labels\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        print 'Fitting random forest classifier with n_classes = %s'%self.n_classes\n",
    "        y_labels = self._get_labels(y)\n",
    "        kf = KFold(n_splits=self.cv, shuffle=False, random_state=random_state)\n",
    "        self.estimators_ = []\n",
    "        # Train individual classifiers\n",
    "        for train, _ in kf.split(X, y_labels):\n",
    "            self.estimators_.append(clone(self.estimator).fit(X[train], y_labels[train]))\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X, y=None):\n",
    "        print 'Applying classifier transformation with n_classes = %s'%self.n_classes\n",
    "        kf = KFold(n_splits=self.cv, shuffle=False, random_state=random_state)\n",
    "        \n",
    "        X_prob = np.zeros((X.shape[0], self.n_classes))\n",
    "        X_pred = np.zeros(X.shape[0])\n",
    "\n",
    "        for estimator, (_, test) in zip(self.estimators_, kf.split(X)):\n",
    "            X_prob[test] = estimator.predict_proba(X[test])\n",
    "            X_pred[test] = estimator.predict(X[test])\n",
    "        return np.hstack([X_prob, np.array([X_pred]).T])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for transforming a row into statistical values\n",
    "def apply_stats_to_row(row):\n",
    "    stats = []\n",
    "    for fun in stat_functions:\n",
    "        stats.append(fun(row))\n",
    "    return stats\n",
    "\n",
    "class StatsTransformer(BaseEstimator, TransformerMixin):\n",
    "    '''\n",
    "    Class describing an object for transforming datasets into statistical values row-wise\n",
    "    '''\n",
    "    def __init__(self, verbose=0, n_jobs=-1, pre_dispatch='2*n_jobs'):\n",
    "        self.verbose = verbose\n",
    "        self.n_jobs = n_jobs\n",
    "        self.pre_dispatch = pre_dispatch\n",
    "    \n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X, y=None):\n",
    "        print 'Applying statistical transformation to dataset...'\n",
    "        parallel = Parallel(n_jobs=self.n_jobs, pre_dispatch=self.pre_dispatch, verbose=self.verbose)\n",
    "        # Get statistics transformation \n",
    "        stats_list = parallel(delayed(apply_stats_to_row)(X[i_smpl, :]) for i_smpl in range(len(X)))\n",
    "        return np.array(stats_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class XGBRegressorCV(BaseEstimator, RegressorMixin):\n",
    "    '''\n",
    "    Class that describes an object that implements XGB Regressor with cross-validation capability\n",
    "    **fit** defines indexes for cross-validation purposes and fits individual XGBRegressor models\n",
    "    '''\n",
    "    def __init__(self, xgb_params=None, fit_params=None, cv=3):\n",
    "        self.xgb_params = xgb_params\n",
    "        self.fit_params = fit_params\n",
    "        self.cv = cv\n",
    "    \n",
    "    @property\n",
    "    def feature_importances_(self):\n",
    "        feature_importances = []\n",
    "        for estimator in self.estimators_:\n",
    "            feature_importances.append(estimator.feature_importances_)\n",
    "        return np.mean(feature_importances, axis=0)\n",
    "    \n",
    "    @property\n",
    "    def evals_result_(self):\n",
    "        evals_result = []\n",
    "        for estimator in self.estimators_:\n",
    "            evals_result.append(estimator.evals_result_)\n",
    "        return np.array(evals_result)\n",
    "    \n",
    "    @property\n",
    "    def best_scores_(self):\n",
    "        best_scores = []\n",
    "        for estimator in self.estimators_:\n",
    "            best_scores.append(estimator.best_score)\n",
    "        return np.array(best_scores)\n",
    "    \n",
    "    @property\n",
    "    def best_score_(self):\n",
    "        return np.mean(self.best_scores_)\n",
    "    \n",
    "    @property\n",
    "    def best_iterations_(self):\n",
    "        best_iterations = []\n",
    "        for estimator in self.estimators_:\n",
    "            best_iterations.append(estimator.best_iteration)\n",
    "        return np.array(best_iterations)\n",
    "    \n",
    "    @property\n",
    "    def best_iteration_(self):\n",
    "        return np.round(np.mean(self.best_iterations_))\n",
    "    \n",
    "    def fit(self, X, y, **fit_params):\n",
    "        print 'Fitting XGB Regressors...'\n",
    "        kf = KFold(n_splits=self.cv, shuffle=False, random_state=random_state)\n",
    "        self.estimators_ = []\n",
    "        \n",
    "        for train, valid in kf.split(X, y):\n",
    "            self.estimators_.append(xgb.XGBRegressor(**self.xgb_params).fit(X[train], y[train], \n",
    "                                                                            eval_set=[(X[valid], y[valid])], \n",
    "                                                                            **self.fit_params))\n",
    "        return self\n",
    "    \n",
    "    def predict(self, X):\n",
    "        print 'Making aggregate XGB Regressor predictions...'\n",
    "        y_pred = []\n",
    "        for estimator in self.estimators_:\n",
    "            y_pred.append(estimator.predict(X))\n",
    "        return np.mean(y_pred, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class _StatFunAdaptor:\n",
    "    '''\n",
    "    Class describing an object that wraps pre-processing functions with a main statistical function\n",
    "    **__init__** sets up the object parameters\n",
    "    **__call__** describes routine steps when object is called\n",
    "    '''\n",
    "    def __init__(self, stat_fun, *funs, **stat_fun_kwargs):\n",
    "        self.stat_fun = stat_fun\n",
    "        self.funs = funs\n",
    "        self.stat_fun_kwargs = stat_fun_kwargs\n",
    "\n",
    "    def __call__(self, x):\n",
    "        x = x[x != 0]  # Only look at nonzero entries\n",
    "        # Transform row with cached functions\n",
    "        for fun in self.funs:\n",
    "            x = fun(x)\n",
    "        if x.size == 0:\n",
    "            return -99999  # Edge case default\n",
    "        return self.stat_fun(x, **self.stat_fun_kwargs)  # Returns result of a run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def diff2(x):\n",
    "    return np.diff(x, n=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_stat_funs():\n",
    "    '''\n",
    "    Function for defining all the statistical functions used for evaluating elements in a row-wise manner\n",
    "    Functions include: length, minimum, maximum, standard deviation, skew, kurtosis, and percentile\n",
    "    '''\n",
    "    stat_funs = []\n",
    "    \n",
    "    stats = [len, np.min, np.max, np.median, np.std, skew, kurtosis] + 19 * [np.percentile]\n",
    "    # Dictionary arguments (nontrivial only for percentile function)\n",
    "    stats_kwargs = [{} for i in range(7)] + [{'q': i} for i in np.linspace(0.05, 0.95, 19)]\n",
    "    \n",
    "    for stat, stat_kwargs in zip(stats, stats_kwargs):\n",
    "        stat_funs.append(_StatFunAdaptor(stat,**stat_kwargs))\n",
    "        stat_funs.append(_StatFunAdaptor(stat, np.diff, **stat_kwargs))  # Apply to 1-diff of row\n",
    "        stat_funs.append(_StatFunAdaptor(stat, diff2, **stat_kwargs))  # Apply to 2-diff of row\n",
    "        stat_funs.append(_StatFunAdaptor(stat, np.unique, **stat_kwargs))  # Apply to unique vals of row\n",
    "        stat_funs.append(_StatFunAdaptor(stat, np.unique, np.diff, **stat_kwargs))  # Apply to unique, 1-diff row vals\n",
    "        stat_funs.append(_StatFunAdaptor(stat, np.unique, diff2, **stat_kwargs))  # Apply to unique, 2-diff row vals\n",
    "    return stat_funs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for retrieving a Random Forest Classifier object\n",
    "def get_rfc():\n",
    "    return RandomForestClassifier(n_estimators=100,\n",
    "                                  max_features=0.5,\n",
    "                                  max_depth=None,\n",
    "                                  max_leaf_nodes=270,\n",
    "                                  min_impurity_decrease=0.0001,\n",
    "                                  random_state=123,\n",
    "                                  n_jobs=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for setting up datasets\n",
    "def get_input(debug=False):\n",
    "    if debug:\n",
    "        print 'Loading debug train and test datasets...'\n",
    "        train = pd.read_csv('../data/train_debug.csv')\n",
    "        test = pd.read_csv('../data/test_debug.csv')\n",
    "    else:\n",
    "        print 'Loading original train and test datasets...'\n",
    "        train = pd.read_csv('../data/train.csv')\n",
    "        test = pd.read_csv('../data/test.csv')\n",
    "    y_train_log = np.log1p(train['target'])\n",
    "    id_test = test['ID']\n",
    "    # Drop unnecessary columns\n",
    "    train.drop(labels=['ID', 'target'], axis=1, inplace=True)\n",
    "    test.drop(labels=['ID'], axis=1, inplace=True)\n",
    "    # Find shape of loaded datasets\n",
    "    print('Shape of training dataset: {} Rows, {} Columns'.format(*train.shape))\n",
    "    print('Shape of test dataset: {} Rows, {} Columns'.format(*test.shape))\n",
    "    \n",
    "    return train.values, y_train_log.values, test.values, id_test.values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main Script:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# XGBoost regressor parameters\n",
    "xgb_params = {'n_estimators': 1000,\n",
    "              'objective': 'reg:linear',\n",
    "              'booster': 'gbtree',\n",
    "              'learning_rate': 0.02,\n",
    "              'max_depth': 22,\n",
    "              'min_child_weight': 57,\n",
    "              'gamma' : 1.45,\n",
    "              'alpha': 0.0,\n",
    "              'lambda': 0.0,\n",
    "              'subsample': 0.67,\n",
    "              'colsample_bytree': 0.054,\n",
    "              'colsample_bylevel': 0.50,\n",
    "              'n_jobs': -1,\n",
    "              'random_state': 456}\n",
    "# Fitting XGB Regressor parameters\n",
    "fit_params = {'early_stopping_rounds': 15, \n",
    "              'eval_metric': 'rmse',\n",
    "              'verbose': False}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = Pipeline([\n",
    "    ('vt', VarianceThreshold(threshold=0.0)),\n",
    "    ('ut', UniqueTransformer()),\n",
    "    ('fu', FeatureUnion([\n",
    "        ('pca', PCA(n_components=100)),\n",
    "        ('ct-2', ClassifierTransformer(get_rfc(), n_classes=2, cv=5)),\n",
    "        ('ct-3', ClassifierTransformer(get_rfc(), n_classes=3, cv=5)),\n",
    "        ('ct-4', ClassifierTransformer(get_rfc(), n_classes=4, cv=5)),\n",
    "        ('ct-5', ClassifierTransformer(get_rfc(), n_classes=5, cv=5)),\n",
    "        ('st', StatsTransformer(verbose=2))\n",
    "    ])),\n",
    "    ('xgb-cv', XGBRegressorCV(xgb_params=xgb_params, fit_params=fit_params, cv=10))\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading debug train and test datasets...\n",
      "Shape of training dataset: 100 Rows, 4992 Columns\n",
      "Shape of test dataset: 200 Rows, 4992 Columns\n"
     ]
    }
   ],
   "source": [
    "# Get data\n",
    "X_train, y_train_log, X_test, id_test = get_input(debug)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define stat functions\n",
    "stat_functions = get_stat_funs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finding unique indexes...\n",
      "Filtering for only unique columns...\n",
      "Fitting random forest classifier with n_classes = 2\n",
      "Applying classifier transformation with n_classes = 2\n",
      "Fitting random forest classifier with n_classes = 3\n",
      "Applying classifier transformation with n_classes = 3\n",
      "Fitting random forest classifier with n_classes = 4\n",
      "Applying classifier transformation with n_classes = 4\n",
      "Fitting random forest classifier with n_classes = 5\n",
      "Applying classifier transformation with n_classes = 5\n",
      "Applying statistical transformation to dataset...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done 100 out of 100 | elapsed:    0.5s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting XGB Regressors...\n",
      "Best CV scores:\n",
      "[1.707402 1.704554 1.366768 1.620533 1.922263 0.790295 1.233099 1.792851\n",
      " 1.094612 1.795743]\n",
      "Averged CV scores:\n",
      "1.502812\n"
     ]
    }
   ],
   "source": [
    "# Train pipeline\n",
    "pipe.fit(X_train, y_train_log)\n",
    "print 'Best CV scores:\\n', pipe.named_steps['xgb-cv'].best_scores_\n",
    "print 'Averged CV scores:\\n', pipe.named_steps['xgb-cv'].best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtering for only unique columns...\n",
      "Applying classifier transformation with n_classes = 2\n",
      "Applying classifier transformation with n_classes = 3\n",
      "Applying classifier transformation with n_classes = 4\n",
      "Applying classifier transformation with n_classes = 5\n",
      "Applying statistical transformation to dataset...\n",
      "Making aggregate XGB Regressor predictions...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done 200 out of 200 | elapsed:    0.8s finished\n"
     ]
    }
   ],
   "source": [
    "# Get predictions\n",
    "y_pred_log = pipe.predict(X_test)\n",
    "y_pred = np.expm1(y_pred_log)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Format submissions\n",
    "submission_path = '../submissions/xgbpipe_0v2_submit.csv'\n",
    "submission = pd.DataFrame()\n",
    "submission['ID'] = id_test\n",
    "submission['target'] = y_pred\n",
    "submission.to_csv(submission_path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

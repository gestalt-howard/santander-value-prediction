{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# Import libraries\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from keras.layers import Input, Dense\n",
    "from keras.models import Model\n",
    "from keras.optimizers import Adam\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import os\n",
    "import json\n",
    "import pdb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper Functions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keras model history class\n",
    "class history(object):\n",
    "    def __init__(self, fname):\n",
    "        '''Defines main parameters for history object'''\n",
    "        self.fname = fname\n",
    "        if os.path.exists(self.fname):\n",
    "            print 'Loading existing history file...'\n",
    "            with open(self.fname, 'r') as handle:\n",
    "                self.history = json.load(handle)\n",
    "        else:\n",
    "            print 'Creating new history file...'\n",
    "            self.history = {'acc': [], 'loss': [], 'val_acc': [], 'val_loss': []}\n",
    "            with open(self.fname, 'w') as handle:\n",
    "                json.dump(self.history, handle)\n",
    "    \n",
    "    \n",
    "    def append_history(self, new_history):\n",
    "        '''Updates history object with new training history'''\n",
    "        for key, value in new_history.iteritems():\n",
    "            for v in value:\n",
    "                self.history[key].append(v)\n",
    "        print 'Saving updated history file...'\n",
    "        with open(self.fname, 'w') as handle:\n",
    "            json.dump(self.history, handle)\n",
    "    \n",
    "    \n",
    "    def visualize_accuracy(self):\n",
    "        '''Plots visualization of training and validation accuracies'''\n",
    "        t_acc = self.history['acc']\n",
    "        v_acc = self.history['val_acc']\n",
    "        # Make plot\n",
    "        plt.figure(figsize=(10, 7))\n",
    "        plt.plot(t_acc, label='training_acc')\n",
    "        plt.plot(v_acc, label='validation_acc')\n",
    "        plt.legend(loc='upper left')\n",
    "        plt.title('Training and Validation Accuracies vs. Epoch Count')\n",
    "        plt.xlabel('Epoch Count')\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get current epoch\n",
    "def get_current_epoch(n_prefix):\n",
    "    files = sorted([f for f in os.listdir(weights_path) if n_prefix in f])\n",
    "    if len(files) == 0:\n",
    "        print 'No epochs have yet been run'\n",
    "        return(-1)\n",
    "    else:\n",
    "        latest_idx = int(files[-1].split(weights_suffix)[0].split('_')[-1])-1\n",
    "        print 'Latest epoch:', latest_idx\n",
    "        return(latest_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Format current epoch\n",
    "def format_epoch(enum):\n",
    "    strnum = str(enum)\n",
    "    fill_val = 3 - len(strnum)\n",
    "    if fill_val > 0:\n",
    "        return(fill_val*str(0)+strnum)\n",
    "    else:\n",
    "        return(strnum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating stacked autoencoders\n",
    "def get_stacked_models(input_size, num_ae):\n",
    "    encoded_input = Input(shape=(input_size,))\n",
    "    autoencoder_list = []\n",
    "    encoder_list = []\n",
    "    for n in range(num_ae):\n",
    "        encoding_dim = int(input_size/(2**(n+1)))  # Size of bottleneck layer\n",
    "        decoding_dim = int(input_size/(2**n))  # Size of reconstruction\n",
    "        # Define autoencoder layers\n",
    "        encoded = Dense(encoding_dim, activation='relu', name='encode_%s'%str(n))(encoded_input)\n",
    "        decoded = Dense(decoding_dim, activation='relu', name='decode_%s'%str(n))(encoded)\n",
    "        # Define and compile models\n",
    "        autoencoder = Model(encoded_input, decoded)\n",
    "        encoder = Model(encoded_input, encoded)\n",
    "        opt = Adam(lr=0.0001)\n",
    "        autoencoder.compile(optimizer=opt, loss='mean_squared_error', metrics=['accuracy'])\n",
    "        autoencoder_list.append(autoencoder)\n",
    "        encoder_list.append(encoder)\n",
    "        # Reset encoded input to smaller size\n",
    "        encoded_input = Input(shape=(encoding_dim,))\n",
    "    return encoder_list, autoencoder_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main Script:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Debug flag:\n",
    "debug=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using debugging datasets\n",
      "Loading training and test dataframes...\n",
      "Training and test dataframes loaded!\n"
     ]
    }
   ],
   "source": [
    "# Load data:\n",
    "data_path = '../../data/'\n",
    "if debug:\n",
    "    print 'Using debugging datasets'\n",
    "    train_path = data_path + 'train_debug.csv'\n",
    "    test_path = data_path + 'test_debug.csv'\n",
    "    epoch_num = 5\n",
    "else:\n",
    "    print 'Using full datasets'\n",
    "    train_path = data_path + 'train.csv'\n",
    "    test_path = data_path + 'test.csv'\n",
    "    epoch_num = 400\n",
    "print 'Loading training and test dataframes...'\n",
    "train_df = pd.read_csv(train_path)\n",
    "test_df = pd.read_csv(test_path)\n",
    "print 'Training and test dataframes loaded!'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "parent_path = os.getcwd() + '/'\n",
    "# Autoencoder weight files metadata\n",
    "weights_path = parent_path + 'weights/'\n",
    "weights_prefix = 'ae_weight_'\n",
    "weights_suffix = '.hdf5'\n",
    "# Autoencoder transformed data metadata\n",
    "trans_path = parent_path + 'data/'\n",
    "trans_prefix = trans_path + 'transformed_'\n",
    "trans_suffix = '.csv'\n",
    "# History file metadata\n",
    "history_prefix = weights_path + 'history_'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating weights folder...\n",
      "Creating transformed data folder...\n"
     ]
    }
   ],
   "source": [
    "# Make folders if they don't exist\n",
    "if not os.path.exists(weights_path):\n",
    "    print 'Creating weights folder...'\n",
    "    os.mkdir(weights_path)\n",
    "if not os.path.exists(trans_path):\n",
    "    print 'Creating transformed data folder...'\n",
    "    os.mkdir(trans_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of training dataset: 100 Rows, 4993 Columns\n",
      "Shape of test dataset: 200 Rows, 4993 Columns\n"
     ]
    }
   ],
   "source": [
    "# Drop training labels\n",
    "train_df.drop(columns=['target'], inplace=True, axis=1)\n",
    "print('Shape of training dataset: {} Rows, {} Columns'.format(*train_df.shape))\n",
    "print('Shape of test dataset: {} Rows, {} Columns'.format(*test_df.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scaling data and assembling master dataframe...\n",
      "Master dataframe assembled\n"
     ]
    }
   ],
   "source": [
    "# Create aggregate train + test dataframe\n",
    "print 'Scaling data and assembling master dataframe...'\n",
    "all_data = pd.concat([train_df, test_df], axis=0)\n",
    "all_data.set_index('ID', inplace=True)\n",
    "# Get columns\n",
    "all_cols = all_data.columns.values\n",
    "all_idx = all_data.index.values\n",
    "# Scale data\n",
    "scaler = StandardScaler()\n",
    "scaled_data = scaler.fit_transform(all_data)\n",
    "scaled_data = pd.DataFrame(data=scaled_data, columns=all_cols, index=all_idx)\n",
    "scaled_data.index.name = 'ID'\n",
    "print 'Master dataframe assembled'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Autoencoder input size: 4992\n"
     ]
    }
   ],
   "source": [
    "# Stacked autoencoder parameters\n",
    "num_ae = 5\n",
    "batch_size = 100\n",
    "num_train = train_df.shape[0]\n",
    "input_size = all_data.shape[1]\n",
    "print 'Autoencoder input size:', input_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create autoencoder and encoder models\n",
    "encoder_list, autoencoder_list = get_stacked_models(input_size, num_ae)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training 0 autoencoder stack...\n",
      "Shape of current data: 300 Rows, 4992 Columns\n",
      "Creating new history file...\n",
      "No epochs have yet been run\n",
      "Train on 300 samples, validate on 300 samples\n",
      "Epoch 1/5\n",
      "300/300 [==============================] - 2s 7ms/step - loss: 1.0084 - acc: 0.0000e+00 - val_loss: 0.9136 - val_acc: 0.0000e+00\n",
      "Epoch 2/5\n",
      "300/300 [==============================] - 1s 5ms/step - loss: 0.8924 - acc: 0.0000e+00 - val_loss: 0.8466 - val_acc: 0.0000e+00\n",
      "Epoch 3/5\n",
      "300/300 [==============================] - 1s 5ms/step - loss: 0.8336 - acc: 0.0000e+00 - val_loss: 0.8082 - val_acc: 0.0033\n",
      "Epoch 4/5\n",
      "300/300 [==============================] - 1s 5ms/step - loss: 0.8014 - acc: 0.0133 - val_loss: 0.7788 - val_acc: 0.0467\n",
      "Epoch 5/5\n",
      "300/300 [==============================] - 1s 5ms/step - loss: 0.7735 - acc: 0.0567 - val_loss: 0.7539 - val_acc: 0.0667\n",
      "Saving updated history file...\n",
      "Latest epoch: 4\n",
      "Shape of current data: 300 Rows, 2496 Columns\n",
      "\n",
      "Training 1 autoencoder stack...\n",
      "Shape of current data: 300 Rows, 2496 Columns\n",
      "Creating new history file...\n",
      "No epochs have yet been run\n",
      "Train on 300 samples, validate on 300 samples\n",
      "Epoch 1/5\n",
      "300/300 [==============================] - 1s 2ms/step - loss: 0.5347 - acc: 0.0067 - val_loss: 0.5127 - val_acc: 0.0033\n",
      "Epoch 2/5\n",
      "300/300 [==============================] - 0s 1ms/step - loss: 0.5056 - acc: 0.0033 - val_loss: 0.4922 - val_acc: 0.0033\n",
      "Epoch 3/5\n",
      "300/300 [==============================] - 0s 1ms/step - loss: 0.4873 - acc: 0.0033 - val_loss: 0.4735 - val_acc: 0.0033\n",
      "Epoch 4/5\n",
      "300/300 [==============================] - 0s 1ms/step - loss: 0.4689 - acc: 0.0033 - val_loss: 0.4565 - val_acc: 0.0033\n",
      "Epoch 5/5\n",
      "300/300 [==============================] - 0s 1ms/step - loss: 0.4503 - acc: 0.0033 - val_loss: 0.4414 - val_acc: 0.0033\n",
      "Saving updated history file...\n",
      "Latest epoch: 4\n",
      "Shape of current data: 300 Rows, 1248 Columns\n",
      "\n",
      "Training 2 autoencoder stack...\n",
      "Shape of current data: 300 Rows, 1248 Columns\n",
      "Creating new history file...\n",
      "No epochs have yet been run\n",
      "Train on 300 samples, validate on 300 samples\n",
      "Epoch 1/5\n",
      "300/300 [==============================] - 0s 975us/step - loss: 0.2725 - acc: 0.0000e+00 - val_loss: 0.2613 - val_acc: 0.0000e+00\n",
      "Epoch 2/5\n",
      "300/300 [==============================] - 0s 407us/step - loss: 0.2566 - acc: 0.0000e+00 - val_loss: 0.2493 - val_acc: 0.0000e+00\n",
      "Epoch 3/5\n",
      "300/300 [==============================] - 0s 409us/step - loss: 0.2456 - acc: 0.0033 - val_loss: 0.2387 - val_acc: 0.0033\n",
      "Epoch 4/5\n",
      "300/300 [==============================] - 0s 426us/step - loss: 0.2358 - acc: 0.0133 - val_loss: 0.2288 - val_acc: 0.0100\n",
      "Epoch 5/5\n",
      "300/300 [==============================] - 0s 418us/step - loss: 0.2254 - acc: 0.0100 - val_loss: 0.2198 - val_acc: 0.0100\n",
      "Saving updated history file...\n",
      "Latest epoch: 4\n",
      "Shape of current data: 300 Rows, 624 Columns\n",
      "\n",
      "Training 3 autoencoder stack...\n",
      "Shape of current data: 300 Rows, 624 Columns\n",
      "Creating new history file...\n",
      "No epochs have yet been run\n",
      "Train on 300 samples, validate on 300 samples\n",
      "Epoch 1/5\n",
      "300/300 [==============================] - 0s 746us/step - loss: 0.1731 - acc: 0.0000e+00 - val_loss: 0.1685 - val_acc: 0.0033\n",
      "Epoch 2/5\n",
      "300/300 [==============================] - 0s 111us/step - loss: 0.1666 - acc: 0.0033 - val_loss: 0.1629 - val_acc: 0.0033\n",
      "Epoch 3/5\n",
      "300/300 [==============================] - 0s 148us/step - loss: 0.1612 - acc: 0.0033 - val_loss: 0.1577 - val_acc: 0.0033\n",
      "Epoch 4/5\n",
      "300/300 [==============================] - 0s 125us/step - loss: 0.1561 - acc: 0.0067 - val_loss: 0.1528 - val_acc: 0.0067\n",
      "Epoch 5/5\n",
      "300/300 [==============================] - 0s 152us/step - loss: 0.1510 - acc: 0.0100 - val_loss: 0.1481 - val_acc: 0.0133\n",
      "Saving updated history file...\n",
      "Latest epoch: 4\n",
      "Shape of current data: 300 Rows, 312 Columns\n",
      "\n",
      "Training 4 autoencoder stack...\n",
      "Shape of current data: 300 Rows, 312 Columns\n",
      "Creating new history file...\n",
      "No epochs have yet been run\n",
      "Train on 300 samples, validate on 300 samples\n",
      "Epoch 1/5\n",
      "300/300 [==============================] - 0s 1ms/step - loss: 0.1049 - acc: 0.0033 - val_loss: 0.1032 - val_acc: 0.0033\n",
      "Epoch 2/5\n",
      "300/300 [==============================] - 0s 64us/step - loss: 0.1024 - acc: 0.0033 - val_loss: 0.1009 - val_acc: 0.0100\n",
      "Epoch 3/5\n",
      "300/300 [==============================] - 0s 69us/step - loss: 0.1002 - acc: 0.0100 - val_loss: 0.0988 - val_acc: 0.0100\n",
      "Epoch 4/5\n",
      "300/300 [==============================] - 0s 58us/step - loss: 0.0981 - acc: 0.0100 - val_loss: 0.0966 - val_acc: 0.0100\n",
      "Epoch 5/5\n",
      "300/300 [==============================] - 0s 73us/step - loss: 0.0959 - acc: 0.0067 - val_loss: 0.0946 - val_acc: 0.0067\n",
      "Saving updated history file...\n",
      "Latest epoch: 4\n",
      "Shape of current data: 300 Rows, 156 Columns\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Train stacked autoencoders\n",
    "current_data = scaled_data\n",
    "history_list = []  # List of history objects\n",
    "\n",
    "visualize_flag = False\n",
    "epoch_idx_params = dict()\n",
    "# Iterate through stacks \n",
    "for n, autoencoder in enumerate(autoencoder_list):\n",
    "    print 'Training %s autoencoder stack...'%n\n",
    "    print('Shape of current data: {} Rows, {} Columns'.format(*current_data.shape))\n",
    "    n_prefix = '%s_%s'%(str(n), weights_prefix)\n",
    "    # Initialize history file\n",
    "    epoch_history = history(history_prefix + str(n) + '.json')\n",
    "    # Load old weights (if exists)\n",
    "    latest_idx = get_current_epoch(n_prefix)\n",
    "    prior_weight_fname = weights_path + n_prefix + format_epoch(latest_idx) + weights_suffix\n",
    "    if os.path.exists(prior_weight_fname):\n",
    "        print 'Loading existing model weight...\\n', prior_weight_fname\n",
    "        autoencoder.load_weights(prior_weight_fname, by_name=True)\n",
    "    # Define performance tracking template\n",
    "    weight_fname = weights_path + n_prefix + '{epoch:03d}' + weights_suffix\n",
    "    checkpoint = ModelCheckpoint(weight_fname, monitor='val_loss', verbose=0, save_best_only=False, \n",
    "                                 save_weights_only=True)\n",
    "    # Train model\n",
    "    train_history = autoencoder.fit(current_data, current_data, epochs=epoch_num, batch_size=batch_size, verbose=1, \n",
    "                              shuffle=True, validation_data=(current_data, current_data), callbacks=[checkpoint], \n",
    "                              initial_epoch=latest_idx+1)\n",
    "    # Update history file with new training history\n",
    "    epoch_history.append_history(train_history.history)\n",
    "    history_list.append(epoch_history)\n",
    "    if visualize_flag:\n",
    "        print 'Visualizing the training and validation accuracies for stage %s'%n\n",
    "        epoch_history.visualize_accuracy()\n",
    "        plt.clf()\n",
    "        current_epoch = int(input('Please specify which epoch to use for predicting (max %s)'%(epoch_num-1)))\n",
    "        encoder_list[n].load_weights(weights_path + n_prefix + format_epoch(current_epoch) + weights_suffix, \n",
    "                                     by_name=True)\n",
    "        epoch_idx_params['param%s'%n] = current_epoch\n",
    "    else:\n",
    "        epoch_idx_params['param%s'%n] = get_current_epoch(n_prefix)\n",
    "    # Transform current data with current encoder layer\n",
    "    current_data = encoder_list[n].predict(current_data)\n",
    "    print('Shape of current data: {} Rows, {} Columns\\n'.format(*current_data.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Latest epoch: 4\n",
      "Loading existing model weight...\n",
      "/Users/cheng-haotai/Projects_Data/santander-value-prediction/scripts/autoencoder/weights/4_ae_weight_004.hdf5\n",
      "Reducing dimensionality for autoencoder layer 0\n",
      "Saving transformed dataset for AE layer 0\n",
      "Latest epoch: 4\n",
      "Loading existing model weight...\n",
      "/Users/cheng-haotai/Projects_Data/santander-value-prediction/scripts/autoencoder/weights/4_ae_weight_004.hdf5\n",
      "Reducing dimensionality for autoencoder layer 1\n",
      "Saving transformed dataset for AE layer 1\n",
      "Latest epoch: 4\n",
      "Loading existing model weight...\n",
      "/Users/cheng-haotai/Projects_Data/santander-value-prediction/scripts/autoencoder/weights/4_ae_weight_004.hdf5\n",
      "Reducing dimensionality for autoencoder layer 2\n",
      "Saving transformed dataset for AE layer 2\n",
      "Latest epoch: 4\n",
      "Loading existing model weight...\n",
      "/Users/cheng-haotai/Projects_Data/santander-value-prediction/scripts/autoencoder/weights/4_ae_weight_004.hdf5\n",
      "Reducing dimensionality for autoencoder layer 3\n",
      "Saving transformed dataset for AE layer 3\n",
      "Latest epoch: 4\n",
      "Loading existing model weight...\n",
      "/Users/cheng-haotai/Projects_Data/santander-value-prediction/scripts/autoencoder/weights/4_ae_weight_004.hdf5\n",
      "Reducing dimensionality for autoencoder layer 4\n",
      "Saving transformed dataset for AE layer 4\n"
     ]
    }
   ],
   "source": [
    "# Create transformations\n",
    "current_data = scaled_data\n",
    "for i, encoder in enumerate(encoder_list):\n",
    "    n_prefix = '%s_%s'%(str(n), weights_prefix)\n",
    "    # Option to select specific epoch\n",
    "    if visualize_flag:\n",
    "        current_epoch = epoch_idx_params['param%s'%i]\n",
    "    else:  # Set latest epoch as current epoch \n",
    "        current_epoch = get_current_epoch(n_prefix)\n",
    "    # Load old weights (if exists)\n",
    "    prior_weight_fname = weights_path + n_prefix + format_epoch(current_epoch) + weights_suffix\n",
    "    if os.path.exists(prior_weight_fname):\n",
    "        print 'Loading existing model weight...\\n', prior_weight_fname\n",
    "        encoder.load_weights(prior_weight_fname, by_name=True)\n",
    "        print 'Reducing dimensionality for autoencoder layer %s'%i\n",
    "        current_data = encoder.predict(current_data)\n",
    "        # Save transformed data\n",
    "        transformed_data = pd.DataFrame(data=current_data, index=all_idx)\n",
    "        print 'Saving transformed dataset for AE layer %s'%i\n",
    "        transformed_data.to_csv(trans_path + 'data_' + str(transformed_data.shape[1]) + trans_suffix, index=None)\n",
    "    else:\n",
    "        print 'No trained epochs exist for autoencoder layer %s'%i\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
